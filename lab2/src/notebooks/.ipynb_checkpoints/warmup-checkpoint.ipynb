{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Warmup\n",
    "\n",
    "## 1. Spark\n",
    "### Basic RDD Operations\n",
    "The following steps demonstrate how to create an RDD from a file and apply transofrmations on it. Let's first creat an RDD, named `pagecounts`, from the input files located in `data/pagecounts`. The files entries will look something like this:\n",
    "```\n",
    "20090507-040000 zh favicon.ico 67 62955\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pagecounts MapPartitionsRDD[5] at textFile at <console>:29"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pagecounts = data/pagecounts MapPartitionsRDD[5] at textFile at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data/pagecounts MapPartitionsRDD[5] at textFile at <console>:29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pagecounts = sc.textFile(\"data/pagecounts\")\n",
    "print(pagecounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `take()` operation of an RDD to get the first K records, e.g., K = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20090505-000000 aa Main_Page 2 9980, 20090505-000000 ab %D0%90%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82 1 465, 20090505-000000 ab %D0%98%D1%85%D0%B0%D0%B4%D0%BE%D1%83_%D0%B0%D0%B4%D0%B0%D2%9F%D1%8C%D0%B0 1 16086, 20090505-000000 af.b Tuisblad 1 36236, 20090505-000000 af.d Tuisblad 4 189738, 20090505-000000 af.q Tuisblad 2 56143, 20090505-000000 af Afrika 1 46833, 20090505-000000 af Afrikaans 2 53577, 20090505-000000 af Australi%C3%AB 1 132432, 20090505-000000 af Barack_Obama 1 23368]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to print the fields is to travers the array and print each record on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20090505-000000 aa Main_Page 2 9980\n",
      "20090505-000000 ab %D0%90%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82 1 465\n",
      "20090505-000000 ab %D0%98%D1%85%D0%B0%D0%B4%D0%BE%D1%83_%D0%B0%D0%B4%D0%B0%D2%9F%D1%8C%D0%B0 1 16086\n",
      "20090505-000000 af.b Tuisblad 1 36236\n",
      "20090505-000000 af.d Tuisblad 4 189738\n",
      "20090505-000000 af.q Tuisblad 2 56143\n",
      "20090505-000000 af Afrika 1 46833\n",
      "20090505-000000 af Afrikaans 2 53577\n",
      "20090505-000000 af Australi%C3%AB 1 132432\n",
      "20090505-000000 af Barack_Obama 1 23368\n"
     ]
    }
   ],
   "source": [
    "for (x <- pagecounts.take(10)) {\n",
    "   println(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `count()` function to see how many records in total are in this data set (this command will take a while, so read ahead while it is running). The pagecounts folder consists of two files, each with around 700K lines, so in total we have around 1400K lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1398882"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second field of each record in the data set is the \"project code\" and contains information about the language of the pages. For example, the project code \"en\" indicates an English page. Let's derive an RDD, named `enPages`, containing only English pages from pagecounts. This can be done by applying a `filter()` function to `pagecounts`. For each record, we can split it by the field delimiter (i.e., a space) and get the second field, and then compare it with the string \"en\". To avoid reading from disks each time we perform any operations on the RDD, we can use `cache()` to cache the RDD into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enPages = MapPartitionsRDD[6] at filter at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "970545"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val enPages = pagecounts.filter(x => x.split(\" \")(1) == \"en\").cache()\n",
    "enPages.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate a histogram of total page views on Wikipedia English pages for the date range represented in our dataset (May 5 to May 7, 2009). The high level idea of what we'll be doing is as follows. First, we generate a key value pair for each line; the key is the date (the first eight characters of the first field), and the value is the number of pageviews for that date (the fourth field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enTuples = MapPartitionsRDD[7] at map at <console>:31\n",
       "enKeyValuePairs = MapPartitionsRDD[8] at map at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at map at <console>:32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val enTuples = enPages.map(x => x.split(\" \"))\n",
    "val enKeyValuePairs = enTuples.map(x => (x(0).substring(0, 8), x(3).toInt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shuffle the data and group all values of the same key together. Finally we sum up the values for each key. There is a convenient method called `reduceByKey` in Spark for exactly this pattern. Note that the second argument to `reduceByKey` determines the number of reducers to use. By default, Spark assumes that the reduce function is commutative and associative and applies combiners on the mapper side. Since we know there is a very limited number of keys in this case (because there are only 3 unique dates in our data set), letâ€™s use only one reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20090507,6175726), (20090505,7076855)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enKeyValuePairs.reduceByKey((x, y) => x + y, 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value RDD Operations\n",
    "The following steps demonstrate how to develop a simple word count application in Spark. For this part, we will use the file located at `data/story/hamlet.txt`. To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `removePunctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase. Since the file is large we use `take(15)`, so that we only print 15 lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "removePunctuation: (text: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.util.matching\n",
    "\n",
    "def removePunctuation(text: String): String = {\n",
    "    text.replaceAll(\"\"\"\\p{Punct}|^\\s+|\\s+$\"\"\", \"\").toLowerCase\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \n",
      "2: 1604\n",
      "3: \n",
      "4: \n",
      "5: the tragedy of hamlet prince of denmark\n",
      "6: \n",
      "7: \n",
      "8: by william shakespeare\n",
      "9: \n",
      "10: \n",
      "11: \n",
      "12: dramatis personae\n",
      "13: \n",
      "14: claudius king of denmark\n",
      "15: marcellus officer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hamletRDD = MapPartitionsRDD[12] at map at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at map at <console>:30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hamletRDD = sc.textFile(\"data/story/hamlet.txt\").map(removePunctuation)\n",
    "hamletRDD.zipWithIndex().take(15).map(x => (x._2 + 1) + \": \" + x._1).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the word count, first we need to o split each line by its spaces. We can apply the string `split()` transformation to split each element of the RDD by its spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33013"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hamletWordsRDD = MapPartitionsRDD[14] at flatMap at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[14] at flatMap at <console>:32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hamletWordsRDD = hamletRDD.flatMap(_.split(\" \"))\n",
    "print(hamletWordsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to filter out the empty elements. Remove all entries where the word is `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31953"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordsRDD = MapPartitionsRDD[15] at filter at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[15] at filter at <console>:34"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordsRDD = hamletWordsRDD.filter(_ != \"\")\n",
    "print(wordsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an RDD that is only words, so let's produce a list of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(zone,1)\n",
      "(youth,16)\n",
      "(yourselves,1)\n",
      "(yourself,15)\n",
      "(yours,6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordsCount = ShuffledRDD[17] at reduceByKey at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[17] at reduceByKey at <console>:36"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordsCount = wordsRDD.map((_, 1)).reduceByKey(_ + _)\n",
    "wordsCount.top(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can take the top 15 words by using the `takeOrdered()` action; however, since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 1090\n",
      "and: 964\n",
      "to: 742\n",
      "of: 675\n",
      "i: 577\n",
      "a: 558\n",
      "you: 554\n",
      "my: 520\n",
      "in: 434\n",
      "it: 419\n",
      "that: 389\n",
      "ham: 358\n",
      "is: 346\n",
      "not: 315\n",
      "his: 304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top15WordsAndCounts = Array((the,1090), (and,964), (to,742), (of,675), (i,577), (a,558), (you,554), (my,520), (in,434), (it,419), (that,389), (ham,358), (is,346), (not,315), (his,304))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(the,1090), (and,964), (to,742), (of,675), (i,577), (a,558), (you,554), (my,520), (in,434), (it,419), (that,389), (ham,358), (is,346), (not,315), (his,304)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top15WordsAndCounts = wordsCount.map(x => (x._2, x._1)).sortByKey().top(15).map(x => (x._2, x._1))\n",
    "top15WordsAndCounts.map(x => x._1 + \": \" + x._2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark SQL\n",
    "The entry point into all functionality in Spark is the `SparkSession` class. To create a basic `SparkSession`, just use `SparkSession.builder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@552ed028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://localhost:4040)\" target=\"new_tab\">Spark UI: local-1537711327464</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1537711327464: Some(http://localhost:4040)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Spark SQL ID2221\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "Now, let's creates a `DataFrame` based on the content of a JSON file, located at `data/people/people.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"data/people/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try more functions on `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Print the schema in a tree format\n",
    "df.printSchema()\n",
    "\n",
    "// Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "\n",
    "// Select everybody, but increment the age by 1\n",
    "df.select(df(\"name\"), df(\"age\") + 1).show()\n",
    "\n",
    "// Select people older than 21\n",
    "df.filter(df(\"age\") > 21).show()\n",
    "\n",
    "// Count people by age\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sql` function on a `SparkSession` enables applications to run SQL queries programmatically and returns the result as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database `global_temp`, and we must use the qualified name to refer it, e.g. `SELECT * FROM global_temp.people`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "// Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "// Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 32|\n",
      "+----+---+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Person\n",
       "caseClassDS = [name: string, age: bigint]\n",
       "peopleDS = [age: bigint, name: string]\n",
       "primitiveDS = [value: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "// Encoders are created for case classes\n",
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "caseClassDS.show()\n",
    "\n",
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val peopleDS = spark.read.json(\"data/people/people.json\").as[Person]\n",
    "peopleDS.show()\n",
    "\n",
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "val primitiveDS = Seq(1, 2, 3).toDS()\n",
    "primitiveDS.map(_ + 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperating with RDDs\n",
    "Spark SQL supports two different methods for converting existing RDDs into Datasets:\n",
    "1. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
    "2. The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime.\n",
    "\n",
    "Let's start with the first method. The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as `Seqs` or `Arrays`. This RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:42: error: stable identifier required, but this.$line7$read.spark.implicits found.\n",
       "       import spark.implicits._\n",
       "                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "import spark.implicits._\n",
    "\n",
    "// Create an instance of SparkContext\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "// Create an RDD of Person objects from a text file, convert it to a Dataframe\n",
    "val peopleRDD = sc.textFile(\"data/people/people.txt\")\n",
    "val peopleDF = peopleRDD.map(_.split(\",\")).map(x => Person(x(0), x(1).trim.toInt)).toDF()\n",
    "\n",
    "// Register the DataFrame as a temporary view\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by Spark\n",
    "val teenagersDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "\n",
    "// The columns of a row in the result can be accessed by field index\n",
    "teenagersDF.map(teenager => \"Name: \" + teenager(0)).show()\n",
    "\n",
    "// or by field name\n",
    "teenagersDF.map(teenager => \"Name: \" + teenager.getAs[String](\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second method we can specify the schema programmatically. When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.\n",
    "1. Create an RDD of Rows from the original RDD.\n",
    "2. Create the schema represented by a `StructType` matching the structure of Rows in the RDD created in Step 1.\n",
    "3. Apply the schema to the RDD of `Row`s via `createDataFrame` method provided by `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "|Name: Michael|\n",
      "|   Name: Andy|\n",
      "| Name: Justin|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "peopleRDD = data/people/people.txt MapPartitionsRDD[40] at textFile at <console>:32\n",
       "schemaString = name age\n",
       "fields = Array(StructField(name,StringType,true), StructField(age,StringType,true))\n",
       "schema = StructType(StructField(name,StringType,true), StructField(age,StringType,true))\n",
       "rowRDD = MapPartitionsRDD[42] at map at <console>:42\n",
       "peopleDF = [name: string, age: string]\n",
       "results = [name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType}\n",
    "\n",
    "// Create an RDD\n",
    "val peopleRDD = spark.sparkContext.textFile(\"data/people/people.txt\")\n",
    "\n",
    "// The schema is encoded in a string\n",
    "val schemaString = \"name age\"\n",
    "\n",
    "// Generate the schema based on the string of schema\n",
    "val fields = schemaString.split(\" \").map(x => StructField(x, StringType, nullable = true))\n",
    "val schema = StructType(fields)\n",
    "\n",
    "// Convert records of the RDD (people) to Rows\n",
    "val rowRDD = peopleRDD.map(_.split(\",\")).map(x => Row(x(0), x(1).trim))\n",
    "\n",
    "// Apply the schema to the RDD\n",
    "val peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "\n",
    "// Creates a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL can be run over a temporary view created using DataFrames\n",
    "val results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "// The results of SQL queries are DataFrames and support all the normal RDD operations\n",
    "// The columns of a row in the result can be accessed by field index or by field name\n",
    "results.map(attributes => \"Name: \" + attributes(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "Spark SQL supports operating on a variety of data sources through the `DataFrame` interface. A `DataFrame` can be operated on as normal RDDs and can also be registered as a temporary table. Registering a DataFrame as a table allows you to run SQL queries over its data. In the simplest form, the default data source (*parquet* unless otherwise configured by `spark.sql.sources.default`) will be used for all operations. Save operations also can optionally take a `SaveMode` that specifies how to handle existing data if present. It can take the values: `error` (default), `append`, `overwrite`, `ignore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pdf = [name: string, favorite_color: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, favorite_color: string ... 1 more field]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load data from a parquet file\n",
    "val pdf = spark.read.load(\"data/people/people.parquet\")\n",
    "pdf.select(\"name\", \"favorite_color\").write.mode(\"ignore\").save(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted.\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n",
       "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
       "  ... 46 elided\n",
       "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 23, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I\n",
       "\tat org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)\n",
       "\tat org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:320)\n",
       "\tat org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\n",
       "\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\n",
       "\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
       "\tat org.apache.parquet.hadoop.CodecFactory$BytesCompressor.compress(CodecFactory.java:112)\n",
       "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:93)\n",
       "\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:150)\n",
       "\tat org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:238)\n",
       "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:121)\n",
       "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:167)\n",
       "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:109)\n",
       "\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:405)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:396)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n",
       "\t... 8 more\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n",
       "  ... 65 more\n",
       "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
       "  ... 3 more\n",
       "Caused by: java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I\n",
       "  at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)\n",
       "  at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:320)\n",
       "  at org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\n",
       "  at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\n",
       "  at org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n",
       "  at org.apache.parquet.hadoop.CodecFactory$BytesCompressor.compress(CodecFactory.java:112)\n",
       "  at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:93)\n",
       "  at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:150)\n",
       "  at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:238)\n",
       "  at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:121)\n",
       "  at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:167)\n",
       "  at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:109)\n",
       "  at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:405)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:396)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Manually specify the data source type, e.g., json, parquet, jdbc.\n",
    "val jdf =  spark.read.format(\"json\").load(\"data/people/people.json\")\n",
    "jdf.select(\"name\", \"age\").write.format(\"parquet\").mode(\"overwrite\").save(\"namesAndAges.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parquet* is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. Let's load data programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: path hdfs://127.0.0.1:9000/user/hrabo/people.parquet already exists.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
       "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "import spark.implicits._\n",
    "\n",
    "val peopleDF = spark.read.json(\"data/people/people.json\")\n",
    "\n",
    "// DataFrames can be saved as Parquet files, maintaining the schema information\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "// Read in the parquet file created above\n",
    "val parquetFileDF = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "// Parquet files can also be used to create a temporary view and then used in SQL statements\n",
    "parquetFileDF.createOrReplaceTempView(\"parquetFile\")\n",
    "val namesDF = spark.sql(\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\")\n",
    "namesDF.map(attributes => \"Name: \" + attributes(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL can automatically infer the schema of a JSON dataset and load it as a `Dataset[Row]`. This conversion can be done using `SparkSession.read.json()` on either a `Dataset[String]`, or a JSON file. Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. For a regular multi-line JSON file, set the `multiLine` option to `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|[Columbus, Ohio]| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "peopleDF = [age: bigint, name: string]\n",
       "teenagerNamesDF = [name: string]\n",
       "otherPeopleDataset = [value: string]\n",
       "otherPeople = [address: struct<city: string, state: string>, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[address: struct<city: string, state: string>, name: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// supported by importing this when creating a Dataset.\n",
    "import spark.implicits._\n",
    "\n",
    "// Read a JSON dataset\n",
    "val peopleDF = spark.read.json(\"data/people/people.json\")\n",
    "\n",
    "// The inferred schema can be visualized using the printSchema() method\n",
    "peopleDF.printSchema()\n",
    "\n",
    "// Creates a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by spark\n",
    "val teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagerNamesDF.show()\n",
    "\n",
    "// Alternatively, a DataFrame can be created for a JSON dataset represented by a Dataset[String] storing \n",
    "// one JSON object per string\n",
    "val otherPeopleDataset = spark.createDataset(\"\"\"{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\"\"\" :: Nil)\n",
    "val otherPeople = spark.read.json(otherPeopleDataset)\n",
    "otherPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
